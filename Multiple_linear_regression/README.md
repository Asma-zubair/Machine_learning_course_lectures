# ğŸ“Š Multiple Linear Regression

This lecture covers **Multiple Linear Regression**, an extension of simple linear regression where **multiple input features** are used to predict a single output variable.

---


## ğŸ¥ Lecture Recording
ğŸ‘‰ [Click here to watch the lecture](https://drive.google.com/file/d/1c5wrmTBe0gFuaqR6RsJo2iT1vgX5JJw4/view?usp=sharing)


---


## ğŸ“‘ Slides
ğŸ‘‰ [Click here to view slides](https://point-made-engine.lovable.app)

---



## ğŸ“Œ What You Will Learn

- Difference between simple and multiple linear regression
- Using more than one feature for prediction
- Regression equation with multiple variables
- Understanding coefficients and bias
- Cost function for multiple regression
- Gradient descent vs Ordinary Least Squares (OLS)
- Practical implementation using sklearn

---

## ğŸ§  Theory Overview

Multiple Linear Regression follows the equation:

\[
y = w_1x_1 + w_2x_2 + w_3x_3 + ... + b
\]

Where:
- **xâ‚, xâ‚‚, xâ‚ƒ** â†’ input features  
- **wâ‚, wâ‚‚, wâ‚ƒ** â†’ corresponding weights  
- **b** â†’ bias term  
- **y** â†’ predicted output  

Each feature contributes to the final prediction.

---

## ğŸ›  Tools & Libraries Used

- Python
- NumPy
- Pandas
- scikit-learn

---

