# ğŸ“Š Polynomial Linear Regression

This lecture covers **Polynomial Linear Regression**, an extension of linear regression used to capture **non-linear relationships** between input features and the output variable.

---

## ğŸ¥ Lecture Recording
ğŸ‘‰ [Click here to watch the lecture](https://drive.google.com/file/d/1BwZLWBEQUK8l8mZaYVoiSob-clSyqRst/view?usp=sharing)

---

## ğŸ“‘ Slides
ğŸ‘‰ [Click here to view slides](https://point-made-engine.lovable.app)

---

## ğŸ“Œ What You Will Learn

- Difference between linear and polynomial regression
- Why polynomial features are needed
- Regression equation with polynomial terms
- Understanding degree of polynomial
- Cost function for polynomial regression
- Gradient descent for training
- Underfitting vs overfitting
- Practical implementation using sklearn

---

## ğŸ§  Theory Overview

Polynomial Linear Regression follows the equation:

y = wâ‚x + wâ‚‚xÂ² + wâ‚ƒxÂ³ + ... + b

Where:
- **x** â†’ input feature
- **xÂ², xÂ³** â†’ polynomial terms
- **wâ‚, wâ‚‚, wâ‚ƒ** â†’ weights
- **b** â†’ bias term
- **y** â†’ predicted output

---

## ğŸ›  Tools & Libraries Used

- Python
- NumPy
- Pandas
- scikit-learn
